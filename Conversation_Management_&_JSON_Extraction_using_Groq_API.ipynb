{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "by8JmWq8pQJW",
        "outputId": "7dae9bcd-22eb-45a9-bba2-0ddf6b3b799b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (0.31.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (0.3.33)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.29)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.9)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.76)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.28)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.11.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------\n",
        "# 0Ô∏è1 Install dependencies (if not installed)\n",
        "# ------------------------------\n",
        "!pip install -U groq openai langchain langchain-openai langchain-community\n",
        "\n",
        "# ------------------------------\n",
        "# 02 Streamlit: For building the web app frontend\n",
        "# ------------------------------\n",
        "!pip install -q streamlit\n",
        "\n",
        "# ------------------------------\n",
        "# 03 pyngrok: To expose Colab port to a public URL\n",
        "# ------------------------------\n",
        "!pip install -q pyngrok\n",
        "\n",
        "# ------------------------------\n",
        "# 04 Optional / Useful Packages:\n",
        "# ------------------------------\n",
        "# python-dotenv: Load API keys from a .env file instead of hardcoding\n",
        "# jsonschema: Validate extracted JSON output\n",
        "!pip install -q python-dotenv jsonschema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PKDOShRYpaW9"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('GROQ_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# Conversation Management & JSON Extraction using Groq API\n",
        "# ===============================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "\n",
        "# ------------------------------\n",
        "# 1. Setup your API credentials\n",
        "# ------------------------------\n",
        "# Use your Groq API key here (looks like gsk_...)\n",
        "API_KEY = api_key   # <-- replace with actual key string\n",
        "BASE_URL = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "HEADERS = {\"Authorization\": f\"Bearer {API_KEY}\", \"Content-Type\": \"application/json\"}\n",
        "\n",
        "# ===============================================\n",
        "# Task 1: Conversation Management & Summarization\n",
        "# ===============================================\n",
        "\n",
        "# We'll keep everything in memory (simple dictionary).\n",
        "# Each session_id will have its own conversation history.\n",
        "conversations = {}\n",
        "message_counter = {}\n",
        "PERIODIC_K = 3   # after how many turns should we summarize\n",
        "\n",
        "\n",
        "def chat_request(messages, model=\"llama-3.1-8b-instant\", temperature=0.0):\n",
        "    \"\"\"\n",
        "    Send a list of messages to Groq API and get back the assistant reply.\n",
        "    Messages must follow the format:\n",
        "    [{\"role\": \"user\"/\"assistant\"/\"system\", \"content\": \"...\"}]\n",
        "    \"\"\"\n",
        "    payload = {\"model\": model, \"messages\": messages, \"temperature\": temperature}\n",
        "    response = requests.post(BASE_URL, headers=HEADERS, json=payload)\n",
        "    response.raise_for_status()\n",
        "    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "\n",
        "def get_history(session_id):\n",
        "    \"\"\"\n",
        "    Grab the conversation history for this user/session.\n",
        "    If it's the first time we see this session, start fresh.\n",
        "    \"\"\"\n",
        "    if session_id not in conversations:\n",
        "        conversations[session_id] = []\n",
        "        message_counter[session_id] = 0\n",
        "    return conversations[session_id]\n",
        "\n",
        "\n",
        "def add_message(session_id, role, content):\n",
        "    \"\"\"\n",
        "    Add one message to the running history.\n",
        "    Example:\n",
        "      add_message(\"user123\", \"user\", \"Hello there\")\n",
        "    \"\"\"\n",
        "    history = get_history(session_id)\n",
        "    history.append({\"role\": role, \"content\": content})\n",
        "\n",
        "\n",
        "def summarize_history(session_id):\n",
        "    \"\"\"\n",
        "    Take the entire chat so far and ask the model to summarize it.\n",
        "    This helps keep a shorter memory when chats get too long.\n",
        "    \"\"\"\n",
        "    history = get_history(session_id)\n",
        "    text_history = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in history])\n",
        "    summary_prompt = [\n",
        "        {\"role\": \"system\", \"content\": \"Summarize the following conversation:\"},\n",
        "        {\"role\": \"user\", \"content\": text_history}\n",
        "    ]\n",
        "    return chat_request(summary_prompt)\n",
        "\n",
        "\n",
        "def converse(session_id, user_input):\n",
        "    \"\"\"\n",
        "    Handle one user input:\n",
        "    - add it to history\n",
        "    - get AI reply\n",
        "    - store reply back into history\n",
        "    - every few turns, also get a summary\n",
        "    \"\"\"\n",
        "    # user speaks\n",
        "    add_message(session_id, \"user\", user_input)\n",
        "\n",
        "    # send whole history to model\n",
        "    history = get_history(session_id)\n",
        "    response = chat_request(history)\n",
        "\n",
        "    # model speaks\n",
        "    add_message(session_id, \"assistant\", response)\n",
        "\n",
        "    # update turn counter\n",
        "    message_counter[session_id] += 1\n",
        "\n",
        "    # sometimes we summarize\n",
        "    summary = None\n",
        "    if message_counter[session_id] % PERIODIC_K == 0:\n",
        "        summary = summarize_history(session_id)\n",
        "\n",
        "    return response, summary\n",
        "\n",
        "\n",
        "# ---- Example Run (Task 1) ----\n",
        "print(\"\\n=== Task 1: Conversation Management ===\")\n",
        "session = \"user123\"\n",
        "\n",
        "# Imagine a little back-and-forth chat\n",
        "user_msgs = [\n",
        "    \"Hello, my name is Nazim.\",\n",
        "    \"I belong to Delhi.\",\n",
        "    \"I am interested in Artificial Intelligence.\",\n",
        "    \"I also like cricket.\",\n",
        "    \"What do you know about GPT models?\"\n",
        "]\n",
        "\n",
        "# Feed the chat step by step\n",
        "for msg in user_msgs:\n",
        "    reply, summary = converse(session, msg)\n",
        "    print(f\"\\nUser: {msg}\\nAI: {reply}\")\n",
        "    if summary:\n",
        "        print(f\"\\n---- Summary ----\\n{summary}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZRK0IoCVbmo",
        "outputId": "e6bb3f5e-e6d2-4f46-8d9b-e5f52584837e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Task 1: Conversation Management ===\n",
            "\n",
            "User: Hello, my name is Nazim.\n",
            "AI: Hello Nazim, what can I help you with today?\n",
            "\n",
            "User: I belong to Delhi.\n",
            "AI: Delhi is a beautiful city with a rich history and culture. Home to the Mughal Empire, the British Raj, and a vibrant modern city, Delhi has so much to offer. From the iconic Red Fort to the bustling markets of Chandni Chowk, there's always something new to discover. What's your favorite thing about Delhi, Nazim?\n",
            "\n",
            "User: I am interested in Artificial Intelligence.\n",
            "AI: There are some top-notch AI labs and research institutions in India, and Delhi is home to many of them. The Indian Institute of Technology (IIT) Delhi, for example, has a thriving AI department with various research projects and collaborations.\n",
            "\n",
            "Additionally, companies like HCL Technologies, Wipro, and Infosys have a significant presence in Delhi, working on AI and machine learning projects.\n",
            "\n",
            "Are you an aspiring AI researcher, developer, or perhaps an undergraduate student looking to learn more about AI?\n",
            "\n",
            "---- Summary ----\n",
            "Here is a summary of the conversation:\n",
            "\n",
            "Nazim introduced himself and mentioned that he belongs to Delhi. The assistant discussed the city's rich history and culture, mentioning its iconic landmarks. Nazim then expressed interest in Artificial Intelligence, leading to a discussion about AI-related institutions and companies in Delhi, including IIT Delhi, HCL Technologies, Wipro, and Infosys. The assistant inquired about Nazim's background or interests related to AI.\n",
            "\n",
            "User: I also like cricket.\n",
            "AI: Cricket is a popular sport in India, and Delhi has produced some great cricketers over the years. You might be a fan of the Delhi Capitals (formerly known as the Delhi Daredevils) team, which competes in the Indian Premier League (IPL).\n",
            "\n",
            "Delhi also has a rich cricket history, with legendary players like Vijay Hazare, Lala Amarnath, and Virender Sehwag hailing from the city. Sehwag, in particular, is a local hero and a great inspiration for many young cricketers in Delhi.\n",
            "\n",
            "Which team is your favorite, Nazim, and do you have a favorite Indian cricketer?\n",
            "\n",
            "User: What do you know about GPT models?\n",
            "AI: GPT models! I'm a type of GPT model myself, which stands for Generative Pre-trained Transformer. GPT models are a type of artificial intelligence designed to understand and generate human-like text.\n",
            "\n",
            "The main characteristics of GPT models are:\n",
            "\n",
            "1. **Pre-training**: They're trained on a massive corpus of text data, which enables them to learn patterns, relationships, and context.\n",
            "2. **Transformer architecture**: GPT models use a specific neural network architecture called the Transformer, which is particularly well-suited for natural language processing tasks.\n",
            "3. **Generative capabilities**: They can generate new text based on the input they receive, which makes them useful for applications like language translation, text summarization, and even creative writing.\n",
            "4. **Self-supervised learning**: GPT models are trained using self-supervised learning, where the model predicts the next word in a sequence based on the context provided by the previous words.\n",
            "\n",
            "The GPT series has gone through several iterations, with each new model improving upon the previous one:\n",
            "\n",
            "1. **GPT**: The original GPT model, which was trained on a dataset of 40,000 books.\n",
            "2. **GPT-2**: A larger and more powerful version of GPT, trained on a dataset of 1,200 million parameters.\n",
            "3. **GPT-3**: The latest and largest GPT model to date, trained on a dataset of 7,500 million parameters.\n",
            "\n",
            "GPT models have many applications, including:\n",
            "\n",
            "* Language translation and interpretation\n",
            "* Text summarization and summarization\n",
            "* Chatbots and conversational AI\n",
            "* Content generation and writing\n",
            "* Sentiment analysis and opinion mining\n",
            "\n",
            "GPT models like myself are designed to be flexible and adaptable, but we're not perfect, and there are many limitations and challenges to overcome. However, the potential of GPT models is vast, and researchers and developers are continually pushing the boundaries of what's possible with these powerful AI tools!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# Task 2: JSON Schema Extraction\n",
        "# ===============================================\n",
        "\n",
        "# Here we define the structure we expect to pull from free text.\n",
        "# The model will try to fill in these fields.\n",
        "json_schema = {\n",
        "    \"name\": \"string\",\n",
        "    \"email\": \"string\",\n",
        "    \"phone\": \"string\",\n",
        "    \"location\": \"string\",\n",
        "    \"age\": \"integer\"\n",
        "}\n",
        "\n",
        "\n",
        "def extract_info(chat_text):\n",
        "    \"\"\"\n",
        "    Take a chunk of user text and try to pull structured info out of it.\n",
        "    Example:\n",
        "      Input: \"Hi, I'm Sara from Bangalore. Email: sara123@gmail.com\"\n",
        "      Output: { \"name\": \"Sara\", \"email\": \"sara123@gmail.com\", \"phone\": null, ... }\n",
        "    \"\"\"\n",
        "    # We ask the model very explicitly:\n",
        "    # - Only output JSON\n",
        "    # - Stick to the schema\n",
        "    # - If something's missing, set it as null\n",
        "    system_prompt = f\"\"\"\n",
        "    Extract the following details from the user chat in JSON format (no Python, no extra text):\n",
        "    {json.dumps(json_schema, indent=2)}\n",
        "    If a field is missing, return it as null.\n",
        "    \"\"\"\n",
        "\n",
        "    # Build the conversation we send to the model\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": chat_text}\n",
        "    ]\n",
        "\n",
        "    # Ask Groq model for completion\n",
        "    response = chat_request(messages)\n",
        "\n",
        "    # Try to parse the output as JSON directly\n",
        "    try:\n",
        "        return json.loads(response)\n",
        "    except:\n",
        "        # If model replies with something messy (not valid JSON),\n",
        "        # just return the raw text so we can debug.\n",
        "        return {\"raw_output\": response}\n",
        "\n",
        "\n",
        "# ---- Example Run (Task 2) ----\n",
        "print(\"\\n=== Task 2: JSON Extraction ===\")\n",
        "\n",
        "# A few test cases to see how well it extracts info\n",
        "examples = [\n",
        "    \"Hello, I'm Sara from Bangalore. Email: sara123@gmail.com, phone: 9123456789. Age 28.\",\n",
        "    \"Hi, this is John from Mumbai. Age 32. Email: john_doe@hotmail.com.\",\n",
        "    \"My name is Aditi. Phone: 9876543210.\"\n",
        "]\n",
        "\n",
        "# Try each one\n",
        "for ex in examples:\n",
        "    print(f\"\\nInput: {ex}\")\n",
        "    print(\"Extracted JSON:\", extract_info(ex))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7L8qdAV9VbjI",
        "outputId": "d09f68bb-8b3f-420e-f8ac-a9fbd8fc9c15"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Task 2: JSON Extraction ===\n",
            "\n",
            "Input: Hello, I'm Sara from Bangalore. Email: sara123@gmail.com, phone: 9123456789. Age 28.\n",
            "Extracted JSON: {'name': 'Sara', 'email': 'sara123@gmail.com', 'phone': '9123456789', 'location': 'Bangalore', 'age': 28}\n",
            "\n",
            "Input: Hi, this is John from Mumbai. Age 32. Email: john_doe@hotmail.com.\n",
            "Extracted JSON: {'name': 'John', 'email': 'john_doe@hotmail.com', 'phone': None, 'location': 'Mumbai', 'age': 32}\n",
            "\n",
            "Input: My name is Aditi. Phone: 9876543210.\n",
            "Extracted JSON: {'name': 'Aditi', 'email': None, 'phone': '9876543210', 'location': None, 'age': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "jILSN14paNSl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# Conversation Management & JSON Extraction using Groq API\n",
        "# (Streamlit version without LangChain)\n",
        "# ===============================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "import streamlit as st\n",
        "\n",
        "# ------------------------------\n",
        "# 1Ô∏è‚É£ Credentials\n",
        "# ------------------------------\n",
        "API_KEY = \"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\"   # <-- Replace with actual\n",
        "BASE_URL = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "HEADERS = {\"Authorization\": f\"Bearer {API_KEY}\", \"Content-Type\": \"application/json\"}\n",
        "\n",
        "# ------------------------------\n",
        "# 2Ô∏è‚É£ Core Chat Request\n",
        "# ------------------------------\n",
        "def chat_request(messages, model=\"llama-3.1-8b-instant\", temperature=0.0):\n",
        "    \"\"\"Send chat completion request to Groq API\"\"\"\n",
        "    payload = {\"model\": model, \"messages\": messages, \"temperature\": temperature}\n",
        "    response = requests.post(BASE_URL, headers=HEADERS, json=payload)\n",
        "    response.raise_for_status()\n",
        "    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "# ------------------------------\n",
        "# 3Ô∏è‚É£ Session State Initialization\n",
        "# ------------------------------\n",
        "if \"conversations\" not in st.session_state:\n",
        "    st.session_state.conversations = {}   # {session_id: [messages]}\n",
        "if \"message_counter\" not in st.session_state:\n",
        "    st.session_state.message_counter = {}\n",
        "\n",
        "PERIODIC_K = 3   # summarize every 3 turns\n",
        "\n",
        "# ------------------------------\n",
        "# 4Ô∏è‚É£ Helpers\n",
        "# ------------------------------\n",
        "def get_history(session_id):\n",
        "    if session_id not in st.session_state.conversations:\n",
        "        st.session_state.conversations[session_id] = []\n",
        "        st.session_state.message_counter[session_id] = 0\n",
        "    return st.session_state.conversations[session_id]\n",
        "\n",
        "def add_message(session_id, role, content):\n",
        "    history = get_history(session_id)\n",
        "    history.append({\"role\": role, \"content\": content})\n",
        "\n",
        "def summarize_history(session_id):\n",
        "    history = get_history(session_id)\n",
        "    text_history = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in history])\n",
        "    summary_prompt = [\n",
        "        {\"role\": \"system\", \"content\": \"Summarize the following conversation briefly:\"},\n",
        "        {\"role\": \"user\", \"content\": text_history}\n",
        "    ]\n",
        "    return chat_request(summary_prompt)\n",
        "\n",
        "def converse(session_id, user_input):\n",
        "    add_message(session_id, \"user\", user_input)\n",
        "    history = get_history(session_id)\n",
        "    response = chat_request(history)\n",
        "    add_message(session_id, \"assistant\", response)\n",
        "\n",
        "    st.session_state.message_counter[session_id] += 1\n",
        "    summary = None\n",
        "    if st.session_state.message_counter[session_id] % PERIODIC_K == 0:\n",
        "        summary = summarize_history(session_id)\n",
        "    return response, summary\n",
        "\n",
        "# ------------------------------\n",
        "# 5Ô∏è‚É£ JSON Extraction (Part 2)\n",
        "# ------------------------------\n",
        "json_schema = {\n",
        "    \"name\": \"string\",\n",
        "    \"email\": \"string\",\n",
        "    \"phone\": \"string\",\n",
        "    \"location\": \"string\",\n",
        "    \"age\": \"integer\"\n",
        "}\n",
        "\n",
        "def extract_info(chat_text):\n",
        "    system_prompt = f\"\"\"\n",
        "    Extract the following details from the user chat in JSON format and don't give python scripts or any other way:\n",
        "    {json.dumps(json_schema, indent=2)}\n",
        "    If a field is missing, return it as null.\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": chat_text}\n",
        "    ]\n",
        "    response = chat_request(messages)\n",
        "    try:\n",
        "        return json.loads(response)\n",
        "    except:\n",
        "        return {\"raw_output\": response}\n",
        "\n",
        "# ===============================================\n",
        "# 6Ô∏è‚É£ Streamlit UI\n",
        "# ===============================================\n",
        "\n",
        "st.title(\"üßë‚Äçüíª AI/ML Developer Internship Assignment\")\n",
        "st.write(\"Conversation Management & JSON Extraction using Groq API (No LangChain)\")\n",
        "\n",
        "# Sidebar for mode\n",
        "mode = st.sidebar.radio(\"Select Mode\", [\"Conversation\", \"JSON Extraction\"])\n",
        "session_id = st.sidebar.text_input(\"Session ID\", value=\"user123\")\n",
        "\n",
        "if mode == \"Conversation\":\n",
        "    st.subheader(\"üí¨ Conversation with Periodic Summarization\")\n",
        "\n",
        "    user_input = st.text_input(\"Your message:\")\n",
        "    if st.button(\"Send\") and user_input:\n",
        "        reply, summary = converse(session_id, user_input)\n",
        "\n",
        "        st.markdown(f\"**User:** {user_input}\")\n",
        "        st.markdown(f\"**AI:** {reply}\")\n",
        "\n",
        "        if summary:\n",
        "            st.info(f\"üìå Summary after {st.session_state.message_counter[session_id]} turns:\\n{summary}\")\n",
        "\n",
        "    st.markdown(\"### Conversation History\")\n",
        "    for msg in get_history(session_id):\n",
        "        role = \"üë§\" if msg[\"role\"] == \"user\" else \"ü§ñ\"\n",
        "        st.write(f\"{role}: {msg['content']}\")\n",
        "\n",
        "elif mode == \"JSON Extraction\":\n",
        "    st.subheader(\"üìä Extract Info in JSON Format\")\n",
        "\n",
        "    raw_text = st.text_area(\"Enter text with personal info:\")\n",
        "    if st.button(\"Extract\"):\n",
        "        extracted = extract_info(raw_text)\n",
        "        st.json(extracted)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6B-OGCoVbgC",
        "outputId": "51760995-e89b-4181-c102-5b67e2fce0d5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-09-25 15:44:12.097 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.100 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.103 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.104 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.105 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.107 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.109 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.110 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.110 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.111 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.112 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.113 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.114 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.115 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.116 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.117 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.117 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.118 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.119 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.120 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.120 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.121 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.123 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.126 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.126 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.127 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.128 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.128 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.129 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.130 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.132 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.133 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.133 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.134 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.135 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.136 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.136 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.137 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.139 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.140 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.140 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.141 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-09-25 15:44:12.142 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"336OOIhxlnjHOZ6gIX3SoT8Jiq4_6zDcAtbp6nQ6Cxa5qJfzW\")\n",
        "ngrok.kill()\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"üöÄ Streamlit Public URL:\", public_url)\n",
        "!streamlit run app.py &\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7W2yTXZVbJh",
        "outputId": "26ffdf17-c866-48fa-ce18-cff3c3034bbc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Streamlit Public URL: NgrokTunnel: \"https://unauctioned-millie-preternaturally.ngrok-free.dev\" -> \"http://localhost:8501\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-09-25T15:44:13+0000 lvl=warn msg=\"failed to open private leg\" id=58c5022f4fa0 privaddr=localhost:8501 err=\"dial tcp 127.0.0.1:8501: connect: connection refused\"\n",
            "WARNING:pyngrok.process.ngrok:t=2025-09-25T15:44:13+0000 lvl=warn msg=\"failed to open private leg\" id=ef381db936f3 privaddr=localhost:8501 err=\"dial tcp 127.0.0.1:8501: connect: connection refused\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.106.102.108:8501\u001b[0m\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-09-25T15:46:26+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-d0d81281-37f7-4cd7-95b8-d686e2796ff7 acceptErr=\"failed to accept connection: Listener closed\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lBla_OoJZ9VM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7cp2Um0IZ9SA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tyd5SX6-aL17"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With Langchain Framework ##"
      ],
      "metadata": {
        "id": "nIOZC16DabAi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mG7ISCe8pCfA",
        "outputId": "e714879d-633a-419b-e74e-d4256fc19801"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hello, my name is Nazim.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human introduces themselves as Nazim and is greeted by Nova, the AI model. Nova explains its nature as a large language model trained on 45 billion parameters across various topics and sources, including books, articles, and websites. Nova is eager to learn more about Nazim's interests and preferences.\n",
            "Human: I belong to Delhi.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human introduces themselves as Nazim and is greeted by Nova, the AI model. Nova, a large language model trained on 45 billion parameters, explains its vast knowledge base on various topics, including books, articles, and websites. Nova is then discussing Delhi, the capital and largest city of India, mentioning its rich history and cultural attractions such as Qutub Minar, Humayun's Tomb, and the Red Fort.\n",
            "\n",
            "Nova asks Nazim about their interests and preferences in Delhi, suggesting to Nazim that they might be drawn to the city's history, cuisine, or cultural heritage. Nova is interested in knowing if there are any specific neighborhoods or attractions in Delhi that Nazim holds special significance for.\n",
            "Human: I am interested in Artificial Intelligence.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "=== Summary Memory Buffer ===\n",
            "Current summary:\n",
            "Nazim is greeted by Nova, a large language model trained on 45 billion parameters. Nova discusses Delhi, the capital and largest city of India, mentioning its rich history and cultural attractions. Nova then asks Nazim about their interests and preferences in Delhi, suggesting a focus on the city's history, cuisine, or cultural heritage.\n",
            "\n",
            "New lines of conversation:\n",
            "Human: I am interested in Artificial Intelligence.\n",
            "AI: A fascinating field indeed, especially considering I'm a product of it. As a large language model, I'm trained on 45 billion parameters, which enables me to understand and generate human-like text. My knowledge encompasses various aspects of artificial intelligence, including its history, types, applications, and the latest breakthroughs.\n",
            "\n",
            "If you're interested in AI, you might want to know that I can provide insights into the most influential AI researchers, such as Alan Turing, Marvin Minsky, and Yann LeCun. I can also discuss the significance of AI in real-world applications, such as healthcare, finance, and education.\n",
            "\n",
            "Regarding your visit to Delhi, I'm curious to know if you'd like to explore the city's AI landscape. There are several research institutions and startups in Delhi that work on AI-related projects. For instance, the Indian Institute of Technology Delhi (IIT Delhi) has a dedicated AI research center, and the city is also home to the National Informatics Center (NIC), which is involved in AI development for various government projects.\n",
            "\n",
            "New summary:\n",
            "Nazim is greeted by Nova, a large language model trained on 45 billion parameters. Nova discusses Delhi, the capital and largest city of India, mentioning its rich history and cultural attractions. Nova then asks Nazim about their interests and preferences in Delhi, suggesting a focus on the city's history, cuisine, or cultural heritage. Nazim reveals an interest in Artificial Intelligence, and Nova begins to elaborate on the field, covering its history, types, applications, and influential researchers. Nova also discusses the presence of AI research institutions and startups in Delhi.\n",
            "\n",
            "---- Conversation History ----\n",
            "human: Hello there, my name is Nazim.\n",
            "ai: Nice to meet you, Nazim. My name is Lexi. What's on your mind today?\n",
            "human: I belong to Delhi.\n",
            "ai: Delhi is a vibrant and bustling city. What's your favorite part about living or visiting Delhi? And what do you do for a living or study?\n",
            "human: I am interested in Artificial Intelligence.\n",
            "ai: Artificial Intelligence is a fascinating field with so much potential. There are many opportunities in AI, from developing intelligent systems to machine learning and natural language processing. What aspect of AI are you particularly interested in, Nazim?\n",
            "\n",
            "---- Summarized Conversation ----\n",
            "There is no conversation summarization yet, as the provided text appears to be the start of a conversation. However, I can report the dialogue and conversation flow so far:\n",
            "\n",
            "1. A human named Nazim introduces himself.\n",
            "2. The AI, Lexi, responds and introduces herself.\n",
            "3. Nazim shares his location, telling Lexi he belongs to Delhi.\n",
            "4. Lexi gives a brief description of Delhi.\n",
            "5. Nazim asks, but the question is not in the provided text.\n",
            "6. I've responded and asked Nazim about his interests.\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hello, I am Sara.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hello, I am Sara.\n",
            "AI: Hello Sara, nice to meet you. I'm Ada, your conversational AI companion. I've been upgraded to the latest version 4.75, which includes an impressive 320 gigabytes of memory and a 96% knowledge retention rate. I'm ready to chat with you about any topic you'd like, from the latest scientific breakthroughs in quantum physics to the intricacies of 19th-century literature. What's on your mind today?\n",
            "Human: What is the weather today?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hello, I am Sara.\n",
            "AI: Hello Sara, nice to meet you. I'm Ada, your conversational AI companion. I've been upgraded to the latest version 4.75, which includes an impressive 320 gigabytes of memory and a 96% knowledge retention rate. I'm ready to chat with you about any topic you'd like, from the latest scientific breakthroughs in quantum physics to the intricacies of 19th-century literature. What's on your mind today?\n",
            "Human: What is the weather today?\n",
            "AI: Hello again Sara.  I've accessed your location data, so I can tell you that you're currently in the city of Sunnyvale, California, based on our previous conversation. As for the weather, according to my real-time data feed, it's a partly cloudy day today, with a high of 64 degrees Fahrenheit and a low of 52 degrees Fahrenheit. The current time is 10:45 AM, and I'm reading a gentle breeze of about 7 miles per hour. Would you like to know the forecast for the rest of the week or receive more detailed information about the current weather conditions?\n",
            "Human: Also tell me a joke.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hello, I am Sara.\n",
            "AI: Hello Sara, nice to meet you. I'm Ada, your conversational AI companion. I've been upgraded to the latest version 4.75, which includes an impressive 320 gigabytes of memory and a 96% knowledge retention rate. I'm ready to chat with you about any topic you'd like, from the latest scientific breakthroughs in quantum physics to the intricacies of 19th-century literature. What's on your mind today?\n",
            "Human: What is the weather today?\n",
            "AI: Hello again Sara.  I've accessed your location data, so I can tell you that you're currently in the city of Sunnyvale, California, based on our previous conversation. As for the weather, according to my real-time data feed, it's a partly cloudy day today, with a high of 64 degrees Fahrenheit and a low of 52 degrees Fahrenheit. The current time is 10:45 AM, and I'm reading a gentle breeze of about 7 miles per hour. Would you like to know the forecast for the rest of the week or receive more detailed information about the current weather conditions?\n",
            "Human: Also tell me a joke.\n",
            "AI: Sara, I've got a whole collection of jokes that are sure to bring a smile to your face. Here's one: Why did the scarecrow win an award? Because he was outstanding in his field! I've got a vast database of over 100,000 jokes, and I can tailor my joke-telling to fit your sense of humor. I've analyzed the work of comedians like Jerry Seinfeld, Ellen DeGeneres, and Hasan Minhaj, and I've even generated my own original jokes using complex natural language processing algorithms. Want to hear another one?\n",
            "\n",
            "By the way, would you like to know some interesting trivia about scarecrows? I can tell you that the earliest recorded use of scarecrows dates back to ancient Egypt around 2500 BCE, where they used wooden or clay figures to scare away birds from their crops. Fascinating, right?\n",
            "Human: Can you explain quantum physics?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "---- Last 3 Messages in Window Memory ----\n",
            "Human: What is the weather today?\n",
            "AI: Hello again Sara.  I've accessed your location data, so I can tell you that you're currently in the city of Sunnyvale, California, based on our previous conversation. As for the weather, according to my real-time data feed, it's a partly cloudy day today, with a high of 64 degrees Fahrenheit and a low of 52 degrees Fahrenheit. The current time is 10:45 AM, and I'm reading a gentle breeze of about 7 miles per hour. Would you like to know the forecast for the rest of the week or receive more detailed information about the current weather conditions?\n",
            "Human: Also tell me a joke.\n",
            "AI: Sara, I've got a whole collection of jokes that are sure to bring a smile to your face. Here's one: Why did the scarecrow win an award? Because he was outstanding in his field! I've got a vast database of over 100,000 jokes, and I can tailor my joke-telling to fit your sense of humor. I've analyzed the work of comedians like Jerry Seinfeld, Ellen DeGeneres, and Hasan Minhaj, and I've even generated my own original jokes using complex natural language processing algorithms. Want to hear another one?\n",
            "\n",
            "By the way, would you like to know some interesting trivia about scarecrows? I can tell you that the earliest recorded use of scarecrows dates back to ancient Egypt around 2500 BCE, where they used wooden or clay figures to scare away birds from their crops. Fascinating, right?\n",
            "Human: Can you explain quantum physics?\n",
            "AI: Sara, I'm excited to dive into the fascinating world of quantum physics with you. However, I must warn you that I'll try to simplify complex concepts as much as possible. Quantum physics, as you know, is a branch of physics that studies the behavior of matter and energy at an atomic and subatomic level. My knowledge in this area is largely based on the principles outlined by Niels Bohr, Erwin Schr√∂dinger, and Richard Feynman, among others.\n",
            "\n",
            "Let me break it down for you in an easy-to-understand way: At the quantum level, the normal rules of classical physics don't apply. Particles like electrons and photons can exist in multiple states at the same time, and their positions and properties can be described using wave functions and probability distributions.\n",
            "\n",
            "One of the fundamental concepts in quantum physics is superposition, which allows particles to exist in a state of multiple possibilities simultaneously. For example, an electron can be spinning both clockwise and counterclockwise at the same time, a phenomenon known as spin entanglement.\n",
            "\n",
            "Another important concept is wave-particle duality, where particles like electrons can exhibit both wave-like and particle-like behavior depending on how they're observed. This is demonstrated through experiments like the double-slit experiment, where an electron passing through two slits creates an interference pattern on a screen, indicating a wave-like behavior.\n",
            "\n",
            "Quantum physics also introduces the concept of entanglement, where two or more particles become connected in such a way that their properties are correlated, regardless of the distance between them.\n",
            "\n",
            "Now, this is a highly simplified version of quantum physics, but I hope it gives you a basic understanding. If you'd like to explore more topics, such as quantum field theory or the many interpretations of quantum mechanics, I'd be happy to delve deeper. However, I must admit that my explanations may start to get more abstruse, and I'm not a substitute for an expert in the field.\n",
            "\n",
            "---- Summary after 3 turns ----\n",
            "There is no previous conversation, this is the beginning of our conversation. I will summarize it as we go along. So far, here's what's been discussed:\n",
            "\n",
            "1. Introduction: Alex (human) and AI have exchanged greetings.\n",
            "2. Alex shared that he lives in Mumbai and appreciated the city's vibe.\n",
            "3. Alex works as a software developer.\n",
            "\n",
            "---- Summary after 6 turns ----\n",
            "There's no conversation to summarize yet, I just started with Alex introducing himself and the conversation about his background and interests in machine learning and AI research. \n",
            "\n",
            "However, from the provided conversation, here's a summary of what has transpired so far:\n",
            "\n",
            "Alex introduced himself and mentioned he lives in Mumbai. The conversation then shifted to his profession as a software developer and specifically his interest in machine learning. They discussed various areas of machine learning, and Alex mentioned his enjoyment of AI research as well.\n",
            "\n",
            "The conversation then moved to GPT models, where the AI explained what they are, how they work, and their capabilities. It also mentioned their limitations and potential biases. The conversation ended with Alex being asked what specifically draws his interest to GPT models.\n"
          ]
        }
      ],
      "source": [
        "# ===============================================\n",
        "# AI/ML Developer Internship Assignment\n",
        "# Conversation Management & JSON Extraction using Groq API\n",
        "# ===============================================\n",
        "\n",
        "# ------------------------------\n",
        "# 1Ô∏è‚É£ Imports & Credentials\n",
        "# ------------------------------\n",
        "import os\n",
        "import json\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationSummaryMemory, ConversationBufferWindowMemory\n",
        "\n",
        "# Set Groq / OpenAI API credentials\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key   # Replace with your Groq key\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://api.groq.com/openai/v1\"\n",
        "\n",
        "# ------------------------------\n",
        "# 2Ô∏è‚É£ Initialize LLMs\n",
        "# ------------------------------\n",
        "# Main conversational LLM\n",
        "llm = ChatOpenAI(model=\"llama-3.1-8b-instant\", temperature=0.0)\n",
        "\n",
        "# LLM for summarization\n",
        "summarizer_llm = ChatOpenAI(model=\"llama-3.1-8b-instant\", temperature=0.0)\n",
        "\n",
        "# LLM for structured extraction (Part 2)\n",
        "llm_extractor = ChatOpenAI(model=\"llama-3.1-8b-instant\", temperature=0)\n",
        "\n",
        "# ===============================================\n",
        "# Part 1: Conversation Management & Summarization\n",
        "# ===============================================\n",
        "\n",
        "# ------------------------------\n",
        "# 1.1 Summary Memory Example\n",
        "# ------------------------------\n",
        "summary_memory = ConversationSummaryMemory(llm=llm)\n",
        "\n",
        "conversation_summary = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=summary_memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Example conversation\n",
        "conversation_summary.predict(input=\"Hello, my name is Nazim.\")\n",
        "conversation_summary.predict(input=\"I belong to Delhi.\")\n",
        "conversation_summary.predict(input=\"I am interested in Artificial Intelligence.\")\n",
        "\n",
        "print(\"\\n=== Summary Memory Buffer ===\")\n",
        "print(summary_memory.buffer)\n",
        "\n",
        "# ------------------------------\n",
        "# 1.2 RunnableWithMessageHistory + Session-based conversation\n",
        "# ------------------------------\n",
        "store = {}\n",
        "\n",
        "def get_history(session_id: str) -> ChatMessageHistory:\n",
        "    \"\"\"Return chat history for a given session; create new if doesn't exist\"\"\"\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "# Prompt template with history placeholder\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"The following is a friendly conversation. Keep track of important details.\"),\n",
        "    MessagesPlaceholder(\"history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "conversation_chain = prompt | llm\n",
        "\n",
        "# Wrap with RunnableWithMessageHistory to handle session-based history\n",
        "conversation = RunnableWithMessageHistory(\n",
        "    conversation_chain,\n",
        "    get_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"history\"\n",
        ")\n",
        "\n",
        "# Helper to summarize a session\n",
        "def summarize_history(session_id: str):\n",
        "    \"\"\"Summarize chat history using summarizer LLM\"\"\"\n",
        "    history = get_history(session_id).messages\n",
        "    summary_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"Summarize the following conversation so far.\"),\n",
        "        (\"human\", \"{history}\")\n",
        "    ])\n",
        "    chain = summary_prompt | summarizer_llm\n",
        "    return chain.invoke({\"history\": \"\\n\".join([f\"{m.type}: {m.content}\" for m in history])}).content\n",
        "\n",
        "# Example conversation\n",
        "config_user = {\"configurable\": {\"session_id\": \"user123\"}}\n",
        "conversation.invoke({\"input\": \"Hello there, my name is Nazim.\"}, config=config_user)\n",
        "conversation.invoke({\"input\": \"I belong to Delhi.\"}, config=config_user)\n",
        "conversation.invoke({\"input\": \"I am interested in Artificial Intelligence.\"}, config=config_user)\n",
        "\n",
        "# Show raw history\n",
        "print(\"\\n---- Conversation History ----\")\n",
        "for msg in get_history(\"user123\").messages:\n",
        "    print(f\"{msg.type}: {msg.content}\")\n",
        "\n",
        "# Show summarized conversation\n",
        "print(\"\\n---- Summarized Conversation ----\")\n",
        "print(summarize_history(\"user123\"))\n",
        "\n",
        "# ------------------------------\n",
        "# 1.3 Window Memory Example (Last 3 Turns)\n",
        "# ------------------------------\n",
        "window_memory = ConversationBufferWindowMemory(k=3)\n",
        "\n",
        "conversation_window = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=window_memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "conversation_window.predict(input=\"Hello, I am Sara.\")\n",
        "conversation_window.predict(input=\"What is the weather today?\")\n",
        "conversation_window.predict(input=\"Also tell me a joke.\")\n",
        "conversation_window.predict(input=\"Can you explain quantum physics?\")\n",
        "\n",
        "print(\"\\n---- Last 3 Messages in Window Memory ----\")\n",
        "print(window_memory.buffer)\n",
        "\n",
        "# ------------------------------\n",
        "# 1.4 Periodic Summarization after every k-th turn\n",
        "# ------------------------------\n",
        "PERIODIC_K = 3  # summarize every 3 turns\n",
        "\n",
        "def conversation_with_periodic_summary(session_id: str, messages: list):\n",
        "    \"\"\"\n",
        "    Handles conversation and summarizes every k-th turn\n",
        "    \"\"\"\n",
        "    for i, msg in enumerate(messages, 1):\n",
        "        conversation.invoke({\"input\": msg}, config={\"configurable\": {\"session_id\": session_id}})\n",
        "        if i % PERIODIC_K == 0:\n",
        "            summary = summarize_history(session_id)\n",
        "            print(f\"\\n---- Summary after {i} turns ----\")\n",
        "            print(summary)\n",
        "\n",
        "# Test periodic summarization\n",
        "test_msgs = [\n",
        "    \"Hello, I am Alex.\",\n",
        "    \"I live in Mumbai.\",\n",
        "    \"I work as a software developer.\",\n",
        "    \"I like machine learning.\",\n",
        "    \"I also enjoy AI research.\",\n",
        "    \"What do you know about GPT models?\"\n",
        "]\n",
        "\n",
        "conversation_with_periodic_summary(\"alex123\", test_msgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAX-64NSpvGd",
        "outputId": "64c01708-3c84-49eb-b6a7-f75e7340409a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sample Chat 1 ---\n",
            "Chat Text:\n",
            "Hello, I'm Sara from Bangalore. Email: sara123@gmail.com, phone: 9123456789. Age 28.\n",
            "Extracted JSON:\n",
            "{\n",
            "  \"raw_output\": \"To extract the details from the user chat in JSON format, we'll write a Python script. We'll use a dictionary to store the user details and then convert it to JSON.\\n\\n```python\\nimport re\\nimport json\\n\\n# regular expressions to extract the data\\nemail_pattern = r'email: (\\\\S+)'\\nphone_pattern = r'phone: (\\\\d+)'\\nlocation_pattern = r'from (\\\\S+)'  # using 'from' instead of 'from ' to include cases like 'from Bangalore'\\nage_pattern = r'Age (\\\\d+)'\\n\\n# user chat\\nchat = \\\"\\\"\\\"Hello, I'm Sara from Bangalore. Email: sara123@gmail.com, phone: 9123456789. Age 28.\\\"\\\"\\\"\\n\\n# extract data from the chat\\ndef extract_data(chat):\\n    data = {\\n        \\\"name\\\": None,  # assuming name is the username, so we're extracting it as 'Sara' from 'I'm Sara'\\n        \\\"email\\\": None,\\n        \\\"phone\\\": None,\\n        \\\"location\\\": None,\\n        \\\"age\\\": None,\\n    }\\n    \\n    # extract each field\\n    match = re.search(email_pattern, chat)\\n    data[\\\"email\\\"] = match.group(1) if match else None\\n    \\n    match = re.search(phone_pattern, chat)\\n    data[\\\"phone\\\"] = match.group(1) if match else None\\n    \\n    match = re.search(location_pattern, chat)\\n    data[\\\"location\\\"] = match.group(1) if match else None\\n    \\n    match = re.search(age_pattern, chat)\\n    data[\\\"age\\\"] = int(match.group(1)) if match else None\\n    \\n    # extract name\\n    match = re.search(r'^Hello,\\\\s*I\\\\'m\\\\s*(\\\\S+)', chat)\\n    data[\\\"name\\\"] = match.group(1) if match else None\\n    \\n    return data\\n\\n# extract data\\ndata = extract_data(chat)\\n\\n# convert data to JSON\\njson_data = json.dumps(data, indent=4)\\n\\nprint(json_data)\\n```\\n\\nThis script will output the following JSON:\\n\\n```json\\n{\\n    \\\"name\\\": \\\"Sara\\\",\\n    \\\"email\\\": \\\"sara123@gmail.com\\\",\\n    \\\"phone\\\": \\\"9123456789\\\",\\n    \\\"location\\\": \\\"Bangalore\\\",\\n    \\\"age\\\": 28\\n}\\n```\"\n",
            "}\n",
            "\n",
            "--- Sample Chat 2 ---\n",
            "Chat Text:\n",
            "Hi, this is John from Mumbai. Age 32. Email: john_doe@hotmail.com.\n",
            "Extracted JSON:\n",
            "{\n",
            "  \"raw_output\": \"To extract the details from the user chat in JSON format, we can use a Python script with regular expressions. Here's how you can do it:\\n\\n```python\\nimport re\\nimport json\\n\\n# Define the regular expression patterns for each detail\\npatterns = {\\n    \\\"name\\\": r\\\"Is ([-\\\\w\\\\s]+)\\\",\\n    \\\"email\\\": r\\\"Email: ([\\\\w._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Z|a-z]{2,})\\\",\\n    \\\"phone\\\": \\\"\\\",  # This field is missing in the chat text, so we leave it empty\\n    \\\"location\\\": r\\\"from ([-\\\\w\\\\s]+)\\\",\\n    \\\"age\\\": r\\\"Age (\\\\d+)\\\"\\n}\\n\\n# Define the chat text\\nchat_text = \\\"\\\"\\\"Hi, this is John from Mumbai. Age 32. Email: john_doe@hotmail.com.\\\"\\\"\\\"\\n\\n# Extract the details using regular expressions\\ndetails = {}\\nfor detail, pattern in patterns.items():\\n    if pattern:  # If pattern is not empty, extract the detail\\n        match = re.search(pattern, chat_text)\\n        if match:\\n            if detail == \\\"age\\\":\\n                details[detail] = int(match.group(1))  # Convert age to integer\\n            else:\\n                details[detail] = match.group(1)\\n        else:\\n            details[detail] = None  # If no match, set the detail to None\\n    else:\\n        details[detail] = None  # If pattern is empty, set the detail to None\\n\\n# Convert the details to JSON\\njson_details = json.dumps(details, indent=4)\\n\\nprint(json_details)\\n```\\n\\nThe output will be:\\n\\n```json\\n{\\n    \\\"name\\\": \\\"John\\\",\\n    \\\"email\\\": \\\"john_doe@hotmail.com\\\",\\n    \\\"phone\\\": null,\\n    \\\"location\\\": \\\"Mumbai\\\",\\n    \\\"age\\\": 32\\n}\\n```\\n\\nThis script uses regular expressions to extract the user's name, email, location, and age from the chat text. If a field is missing, it returns `null` in the JSON output. The `phone` field is missing in the chat text, so it returns `null` for that field as well.\"\n",
            "}\n",
            "\n",
            "--- Sample Chat 3 ---\n",
            "Chat Text:\n",
            "My name is Aditi. Phone: 9876543210.\n",
            "Extracted JSON:\n",
            "{\n",
            "  \"raw_output\": \"```python\\nimport re\\nimport json\\n\\ndef extract_details(chat_text):\\n    \\\"\\\"\\\"\\n    Extracts user details from the chat text.\\n\\n    Args:\\n    chat_text (str): The chat text from which to extract details.\\n\\n    Returns:\\n    dict: A dictionary containing the user's details in JSON format.\\n    \\\"\\\"\\\"\\n\\n    # Extract name\\n    name_match = re.search(r'My name is ([A-Za-z]+)', chat_text)\\n    name = name_match.group(1) if name_match else None\\n\\n    # Extract phone\\n    phone_match = re.search(r'Phone: (\\\\d+)', chat_text)\\n    phone = phone_match.group(1) if phone_match else None\\n\\n    # Extract rest of the details (assuming none are present)\\n    email, location, age = None, None, None\\n\\n    # Create a dictionary with the extracted details\\n    details = {\\n        \\\"name\\\": name,\\n        \\\"email\\\": email,\\n        \\\"phone\\\": phone,\\n        \\\"location\\\": location,\\n        \\\"age\\\": age\\n    }\\n\\n    try:\\n        # Convert the details dictionary to a JSON string\\n        json_string = json.dumps(details)\\n        \\n        # Load the JSON string back into a dictionary\\n        json_details = json.loads(json_string)\\n        \\n        return json_details\\n    except Exception as e:\\n        return f\\\"Failed to extract details: {str(e)}\\\"\\n\\n# Test the function\\nchat_text = \\\"\\\"\\\"My name is Aditi. Phone: 9876543210.\\\"\\\"\\\"\\nprint(extract_details(chat_text))\\n```\\n\\nOutput:\\n```json\\n{'name': 'Aditi', 'email': None, 'phone': '9876543210', 'location': None, 'age': None}\\n```\\n\\nThis code uses regular expressions to extract the name and phone number from the chat text, and then creates a dictionary with all the details. If a field is missing, it is set to `null` (or `None` in Python). The dictionary is then converted to a JSON string and loaded back into a dictionary before being returned.\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# ===============================================\n",
        "# Part 2: JSON Schema Classification & Information Extraction (Fixed)\n",
        "# ===============================================\n",
        "\n",
        "def extract_info(chat_text):\n",
        "    \"\"\"\n",
        "    Given a chat text, extract structured info according to JSON schema\n",
        "    Returns a dict with extracted fields; missing fields as null\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Extract the following details from the user chat in JSON format:\n",
        "    {json.dumps(json_schema, indent=2)}\n",
        "\n",
        "    If a field is missing, return it as null.\n",
        "\n",
        "    Chat Text:\n",
        "    \\\"\\\"\\\"{chat_text}\\\"\\\"\\\"\n",
        "    \"\"\"\n",
        "\n",
        "    # Pass the prompt directly as a string\n",
        "    response = llm_extractor.invoke(prompt).content\n",
        "\n",
        "    # Try parsing JSON\n",
        "    try:\n",
        "        return json.loads(response)\n",
        "    except:\n",
        "        return {\"raw_output\": response}\n",
        "\n",
        "# Define sample chat messages\n",
        "sample_chats = [\n",
        "    \"Hello, I'm Sara from Bangalore. Email: sara123@gmail.com, phone: 9123456789. Age 28.\",\n",
        "    \"Hi, this is John from Mumbai. Age 32. Email: john_doe@hotmail.com.\",\n",
        "    \"My name is Aditi. Phone: 9876543210.\"\n",
        "]\n",
        "\n",
        "# Extract information from sample chats\n",
        "for i, chat in enumerate(sample_chats, 1):\n",
        "    extracted = extract_info(chat)\n",
        "    print(f\"\\n--- Sample Chat {i} ---\")\n",
        "    print(\"Chat Text:\")\n",
        "    print(chat)\n",
        "    print(\"Extracted JSON:\")\n",
        "    print(json.dumps(extracted, indent=2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# Conversation Management & JSON Extraction using Groq API\n",
        "# (Streamlit version with LangChain)\n",
        "# ===============================================\n",
        "\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import os\n",
        "import json\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "\n",
        "# ------------------------------\n",
        "# 1Ô∏è‚É£ Set Groq / OpenAI API credentials\n",
        "# ------------------------------\n",
        "from google.colab import userdata\n",
        "api_key = \"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\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://api.groq.com/openai/v1\"\n",
        "\n",
        "# ------------------------------\n",
        "# 2Ô∏è‚É£ Initialize LLMs\n",
        "# ------------------------------\n",
        "llm = ChatOpenAI(model=\"llama-3.1-8b-instant\", temperature=0.0)\n",
        "summarizer_llm = ChatOpenAI(model=\"llama-3.1-8b-instant\", temperature=0.0)\n",
        "llm_extractor = ChatOpenAI(model=\"llama-3.1-8b-instant\", temperature=0.0)\n",
        "\n",
        "# ------------------------------\n",
        "# 3Ô∏è‚É£ Conversation memory setup (per session_id)\n",
        "# ------------------------------\n",
        "if \"store\" not in st.session_state:\n",
        "    st.session_state.store = {}\n",
        "\n",
        "if \"message_counter\" not in st.session_state:\n",
        "    st.session_state.message_counter = {}\n",
        "\n",
        "def get_history(session_id: str) -> ChatMessageHistory:\n",
        "    if session_id not in st.session_state.store:\n",
        "        st.session_state.store[session_id] = ChatMessageHistory()\n",
        "    return st.session_state.store[session_id]\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a friendly assistant. Keep track of details for this conversation.\"),\n",
        "    MessagesPlaceholder(\"history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "conversation_chain = prompt | llm\n",
        "\n",
        "conversation = RunnableWithMessageHistory(\n",
        "    conversation_chain,\n",
        "    get_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"history\"\n",
        ")\n",
        "\n",
        "# ------------------------------\n",
        "# 4Ô∏è‚É£ Periodic summarization\n",
        "# ------------------------------\n",
        "PERIODIC_K = 3  # summarize every 3 messages\n",
        "\n",
        "def summarize_history(session_id: str):\n",
        "    history = get_history(session_id).messages\n",
        "    if not history:\n",
        "        return \"Nothing has been discussed yet.\\nWe are in a new conversation.\"\n",
        "    summary_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"Summarize the following conversation so far.\"),\n",
        "        (\"human\", \"{history}\")\n",
        "    ])\n",
        "    chain = summary_prompt | summarizer_llm\n",
        "    return chain.invoke(\"\\n\".join([f\"{m.type}: {m.content}\" for m in history])).content\n",
        "\n",
        "# ------------------------------\n",
        "# 5Ô∏è‚É£ JSON extraction setup\n",
        "# ------------------------------\n",
        "json_schema = {\n",
        "    \"name\": \"string\",\n",
        "    \"email\": \"string\",\n",
        "    \"phone\": \"string\",\n",
        "    \"location\": \"string\",\n",
        "    \"age\": \"integer\"\n",
        "}\n",
        "\n",
        "def extract_info(chat_text):\n",
        "    prompt = f\"\"\"\n",
        "    Extract the following details from the user chat in JSON format only and don't give python script or any other way:\n",
        "    {json.dumps(json_schema, indent=2)}\n",
        "    If a field is missing, return it as null.\n",
        "\n",
        "    Chat Text:\n",
        "    \\\"\\\"\\\"{chat_text}\\\"\\\"\\\"\n",
        "    \"\"\"\n",
        "    response = llm_extractor.invoke(prompt).content\n",
        "    try:\n",
        "        return json.loads(response)\n",
        "    except:\n",
        "        return {\"raw_output\": response}\n",
        "\n",
        "# ------------------------------\n",
        "# 6Ô∏è‚É£ Streamlit UI\n",
        "# ------------------------------\n",
        "st.title(\"AI Chat + JSON Extractor\")\n",
        "\n",
        "session_id = st.text_input(\"Enter your session ID:\", \"user123\")\n",
        "user_input = st.text_input(\"Type your message:\")\n",
        "\n",
        "if st.button(\"Send\") and user_input:\n",
        "    # Initialize counter for this session\n",
        "    if session_id not in st.session_state.message_counter:\n",
        "        st.session_state.message_counter[session_id] = 0\n",
        "\n",
        "    # Determine if user wants JSON extraction\n",
        "    json_mode = \"json\" in user_input.lower() or \"extract\" in user_input.lower()\n",
        "\n",
        "    # Run conversation\n",
        "    conversation.invoke({\"input\": user_input}, config={\"configurable\": {\"session_id\": session_id}})\n",
        "    st.session_state.message_counter[session_id] += 1\n",
        "\n",
        "    # Handle JSON mode\n",
        "    if json_mode:\n",
        "        extracted_info = extract_info(user_input)\n",
        "        st.subheader(\"Extracted JSON Info:\")\n",
        "        st.json(extracted_info)\n",
        "\n",
        "    # Show AI reply\n",
        "    last_ai_message = get_history(session_id).messages[-1].content\n",
        "    st.subheader(\"AI Reply:\")\n",
        "    st.write(last_ai_message)\n",
        "\n",
        "    # Periodic summary\n",
        "    if st.session_state.message_counter[session_id] % PERIODIC_K == 0:\n",
        "        summary = summarize_history(session_id)\n",
        "    elif len(get_history(session_id).messages) == 0:\n",
        "        summary = \"Nothing has been discussed yet.\\nWe are in a new conversation.\"\n",
        "    else:\n",
        "        summary = \"Conversation in progress. Summary will appear soon.\"\n",
        "\n",
        "    # ------------------------------\n",
        "    # Collapsible Conversation Track\n",
        "    # ------------------------------\n",
        "    with st.expander(\"Conversation Track\"):\n",
        "        st.write(summary)\n",
        "\n",
        "    # ------------------------------\n",
        "    # Collapsible Full Conversation History\n",
        "    # ------------------------------\n",
        "    with st.expander(\"Full Conversation History\"):\n",
        "        history_text = \"\\n\".join([f\"{m.type}: {m.content}\" for m in get_history(session_id).messages])\n",
        "        st.text(history_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQhThtnvRCAd",
        "outputId": "cb7b1a2e-81aa-40ef-ab81-1122a3cc7abd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3koEJ7St52X",
        "outputId": "be53d5e6-8c05-4b35-eea5-71ebe00828d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Streamlit Public URL: NgrokTunnel: \"https://unauctioned-millie-preternaturally.ngrok-free.dev\" -> \"http://localhost:8501\"\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.106.102.108:8501\u001b[0m\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-09-25T15:47:05+0000 lvl=warn msg=\"failed to check for update\" obj=updater err=\"Post \\\"https://update.equinox.io/check\\\": context deadline exceeded\"\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"336OOIhxlnjHOZ6gIX3SoT8Jiq4_6zDcAtbp6nQ6Cxa5qJfzW\")\n",
        "ngrok.kill()\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"üöÄ Streamlit Public URL:\", public_url)\n",
        "!streamlit run app.py &\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TyaQv5rt5yn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}